# -*- coding: utf-8 -*-
"""Water_Quality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19KM46AWOPFt2ABoVuyIoW-Qzcmpc97mO
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
#%matplotlib inline
from matplotlib import style
import graphviz
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve
from sklearn.tree import DecisionTreeClassifier, export_graphviz

from google.colab import files
uploaded = files.upload()

import io 
df2 = pd.read_excel(io.BytesIO(uploaded['Water_Potability.xlsx']))
df = pd.read_excel("Water_Potability.xlsx")
df

df.shape # show có bao nhiêu dòng dữ liệu và thuộc tính

df.isnull().sum() #show dòng dữ liệu trống

#show biểu đồ dữ liệu trống
style.use('ggplot')
df.isnull().sum().sort_values(ascending=False).plot(kind='barh',color='blue',label='Missing Values',alpha=0.6)
plt.legend()

df.info()

df.describe() 
# mô tả thông tin. Thống kê dữ liệu trong pandas
# mean = trung bình
# std = độ lệch chuẩn

pH_nan_1 = df.query('Potability == 1')['ph'][df['ph'].isna()].index

df.loc[pH_nan_1,'ph'] =df.query('Potability == 1')['ph'][df['ph'].notna()].mean()

pH_nan_0 = df.query('Potability == 0')['ph'][df['ph'].isna()].index
df.loc[pH_nan_0,'ph'] = df.query('Potability == 0')['ph'][df['ph'].notna()].mean()

Sulfate_nan_1 = df.query('Potability == 1')['Sulfate'][df['Sulfate'].isna()].index
df.loc[Sulfate_nan_1,'Sulfate'] =df.query('Potability == 1')['Sulfate'][df['Sulfate'].notna()].mean()

Sulfate_nan_0 = df.query('Potability == 0')['Sulfate'][df['Sulfate'].isna()].index
df.loc[Sulfate_nan_0,'Sulfate'] = df.query('Potability == 0')['Sulfate'][df['Sulfate'].notna()].mean()

df=df.dropna(subset=["Trihalomethanes"])

df.isnull().sum()

df

Potability=df["Potability"].value_counts()
Potability

sns.distplot(df['ph'])
#plt.savefig("ph.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

sns.distplot(df['Hardness'])
#plt.savefig("Hardness.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

sns.distplot(df['Solids'])
#plt.savefig("Solids.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

sns.distplot(df['Chloramines'])
#plt.savefig("Chloramines.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

sns.distplot(df['Sulfate'])
#plt.savefig("Sulfate.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

sns.distplot(df['Conductivity'])
#plt.savefig("Conductivity.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

sns.distplot(df['Organic_carbon'])
#plt.savefig("Organic_carbon.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

sns.distplot(df['Trihalomethanes'])
#plt.savefig("Trihalomethanes.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

sns.distplot(df['Turbidity'])
#plt.savefig("Turbidity.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

sns.distplot(df['Potability'])
#plt.savefig("Potability.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

df.hist(figsize=(14,14))
plt.show()

#Visualize biểu đồ Violin
# Biểu đồ violin dưới đây thể hiện sự phân bố của các đặc điểm phân loại. 
#Có thể thấy thuộc tính "Solids" của nước, có thể góp phần vào việc phân loại.
df_div = pd.melt(df, "Potability", var_name="Characteristics")
fig, ax = plt.subplots(figsize=(16,6))
p = sns.violinplot(ax = ax, x="Characteristics", y="value", hue="Potability", split = True, data=df_div, inner = 'quartile', palette = 'Set2')
df_no_class = df.drop(["Potability"],axis = 1)
p.set_xticklabels(rotation = 90, labels = list(df_no_class.columns));
#plt.savefig("violinplot.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

#Visualize biểu đồ thể hiện sự tương quan giữa các thuộc tính
#Nhận xét: Thông thường biến ít tương quan nhất là biến quan trọng nhất để phân loại. 
#Trong trường hợp này, "Solids" = -0,082. Vì vậy chúng ta hãy xem xét kỹ nó
plt.figure(figsize=(14,12))
sns.heatmap(df.corr(),linewidths=.1,cmap="YlGnBu", annot=True, annot_kws={"size": 8})
plt.yticks(rotation=0);
#plt.savefig("corr.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

Accuracy={}

X_dt = df.drop(['Potability'], axis=1)  
Y = df["Potability"]
X_train_dt, X_test_dt, Y_train_dt, Y_test_dt = train_test_split(X_dt, Y, random_state=42, test_size=0.3) #train 70% - test 30%

"""# **CLASSIFICATION**

# **THUẬT TOÁN DECISION**
"""

def train_and_test(model, train_data, test_data):
    """
    Trains the model on the training data, prints the score for such model evaluating it on the testing data
    and also returns the learned model
    model: classifier to be used
    train_data: [X_train_dt, Y_train_dt]
    test_data: [X_test_dt, Y_test_dt]
    """
    # Fits the model to the training data 
    model.fit(train_data[0], train_data[1])
    # Evaluates the model using the testing data
    print(model.score(test_data[0], test_data[1]))
    # Returns the fitted model
    return model

from sklearn.tree import DecisionTreeClassifier as clf
Y_name = ['1','0']
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X_train_dt, Y_train_dt)
Y_pred_dtc = clf.predict(X_test_dt)
dot_data = export_graphviz(clf, out_file=None, 
                         feature_names=X_dt.columns,
                         class_names=Y_name, 
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = graphviz.Source(dot_data)
#graph.render(filename='DecisionTree')
graph

#Visualize biểu đồ thể hiện các đặc tính quan trọng trong bộ dữ liệu Water_Quality
features_list = X_dt.columns.values
feature_importance = clf.feature_importances_
sorted_idx = np.argsort(feature_importance)

plt.figure(figsize=(6,5))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), features_list[sorted_idx])
plt.xlabel('Importance')
plt.title('Feature importances')
plt.draw()
plt.savefig("featureimp.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

from sklearn import tree
clf=tree.DecisionTreeClassifier(random_state=42)
clf.fit(X_train_dt,Y_train_dt)
Y_pred_dtc=clf.predict(X_test_dt)

confusion_matrix = pd.crosstab(Y_test_dt, Y_pred_dtc, rownames=['Actual'], colnames=['Predicted'])
sns.heatmap(confusion_matrix, annot=True, cmap="YlGnBu" ,fmt='g')
plt.savefig("rdfcm.png", format='png', dpi=150, bbox_inches='tight')

from sklearn.model_selection import cross_val_predict, cross_val_score

print("TEST RESULTS:\n")
#Report
print('Decision Tree Classifier Report:\n\n{}\n'.format(classification_report(Y_test_dt, Y_pred_dtc)))
res = cross_val_score(clf, X_test_dt,Y_test_dt, cv=10, n_jobs=-1, scoring='accuracy')
#Độ chính xác trung bình
print('Average Accuracy:\t{0:.4f}\n'.format((res.mean())))
#Độ lệch chuẩn
print('Standard Deviation:\t{0:.4f}\n'.format(res.std()))

from sklearn import metrics
Accuracy_Decision_Tree=round((metrics.accuracy_score(Y_test_dt, Y_pred_dtc)*100),2)
print('Accuracy: ',Accuracy_Decision_Tree,"%")
Accuracy["tree"]=Accuracy_Decision_Tree

#Visualize đồ thị đường cong PR (Precision-Recall Curves)
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve
precision, recall, thresholds = precision_recall_curve(Y_test_dt, Y_pred_dtc)
area = auc(recall, precision)
plt.figure()
plt.plot(recall, precision, label = 'Area Under Curve = %0.3f'% area)
plt.legend(loc = 'lower left')
plt.title('Precision-Recall Curves of Decision Tree')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([-0.1, 1.1])
plt.xlim([-0.1, 1.1])
plt.savefig("dtpr.png", format='png', dpi=150, bbox_inches='tight')
plt.show()

"""# **THUẬT TOÁN RANDOM FOREST**"""

from sklearn.ensemble import RandomForestClassifier
rdf= RandomForestClassifier()
rdf.fit(X_train_dt,Y_train_dt)
Y_pred_rdf=rdf.predict(X_test_dt)

confusion_matrix = pd.crosstab(Y_test_dt, Y_pred_rdf, rownames=['Actual'], colnames=['Predicted'])
sns.heatmap(confusion_matrix, annot=True, cmap="YlGnBu" ,fmt='g')
#plt.savefig("rdfcm.png", format='png', dpi=150, bbox_inches='tight')

from sklearn.metrics import accuracy_score
from sklearn import metrics
Accuracy_RandomForestClassifier=round((accuracy_score(Y_test_dt,Y_pred_rdf)*100),2)
print("Accuracy_RandomForestClassifier : ",Accuracy_RandomForestClassifier,"%")
Accuracy["RandomForestClassifier"]=Accuracy_RandomForestClassifier

from sklearn.model_selection import cross_val_predict, cross_val_score

print("TEST RESULTS:\n")
#Report
print('Random Forest Classifier Report:\n\n{}\n'.format(classification_report(Y_test_dt, Y_pred_rdf)))
res = cross_val_score(rdf, X_test_dt,Y_test_dt, cv=10, n_jobs=-1, scoring='accuracy')
#Độ chính xác trung bình
print('Average Accuracy:\t{0:.4f}\n'.format((res.mean())))
#Độ lệch chuẩn
print('Standard Deviation:\t{0:.4f}\n'.format(res.std()))

#Visualize đồ thị đường cong PR (Precision-Recall Curves)
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve
precision, recall, thresholds = precision_recall_curve(Y_test_dt, Y_pred_rdf)
area = auc(recall, precision)
plt.figure()
plt.plot(recall, precision, label = 'Area Under Curve = %0.3f'% area)
plt.legend(loc = 'lower left')
plt.title('Precision-Recall Curves of Random Forest')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([-0.1, 1.1])
plt.xlim([-0.1, 1.1])
#plt.savefig("dtpr.png", format='png', dpi=150, bbox_inches='tight')
plt.show()

from sklearn.tree import export_graphviz
import matplotlib.pyplot as plt
from sklearn import tree

plt.figure(figsize=(100,100))
plt.title("Tree 1 Visualization")
_ = tree.plot_tree(rdf.estimators_[0], feature_names=X_dt.columns, filled=True)

"""# **THUẬT TOÁN SUPPOR VECTOR MACHINE**"""

from sklearn.svm import SVC
svm = SVC(random_state=42, gamma="auto", probability=True)
svm.fit(X_train_dt,Y_train_dt)

Y_pred_svm = svm.predict(X_test_dt)

Accuracy_Svm=round((metrics.accuracy_score(Y_test_dt, Y_pred_svm)*100),2)
print('Accuracy: ',Accuracy_Svm,"%")
Accuracy["SVC"]=Accuracy_Svm

from sklearn.model_selection import cross_val_predict, cross_val_score

print("TEST RESULTS:\n")
#Report
print('Random Forest Classifier Report:\n\n{}\n'.format(classification_report(Y_test_dt, Y_pred_svm)))
res = cross_val_score(svm, X_test_dt,Y_test_dt, cv=10, n_jobs=-1, scoring='accuracy')
#Độ chính xác trung bình
print('Average Accuracy:\t{0:.4f}\n'.format((res.mean())))
#Độ lệch chuẩn
print('Standard Deviation:\t{0:.4f}\n'.format(res.std()))

confusion_matrix = pd.crosstab(Y_test_dt, Y_pred_svm, rownames=['Actual'], colnames=['Predicted'])
sns.heatmap(confusion_matrix, annot=True, cmap="YlGnBu" ,fmt='g')
plt.savefig("svmcm.png", format='png', dpi=150, bbox_inches='tight')

#Visualize đồ thị đường cong PR (Precision-Recall Curves)
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve
precision, recall, thresholds = precision_recall_curve(Y_test_dt, Y_pred_svm)
area = auc(recall, precision)
plt.figure()
plt.plot(recall, precision, label = 'Area Under Curve = %0.3f'% area)
plt.legend(loc = 'lower left')
plt.title('Precision-Recall Curves of Support Vector Machine')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([-0.1, 1.1])
plt.xlim([-0.1, 1.1])
plt.savefig("dtpr5.png", format='png', dpi=150, bbox_inches='tight')

"""# **CLUSTERING**"""

# Chọn thuộc tính quan trọng để phân cụm
features = ["Solids","Sulfate","Potability"]
df2 = df.dropna(subset=features)
data = df2[features].copy()
data

data_clus_dt = data.drop(['Potability'], axis=1)
Y_clus = data["Potability"]
X_train_clus_dt, X_test_clus_dt, Y_train_clus_dt, Y_test_clus_dt = train_test_split(data_clus_dt, Y_clus, random_state=42, test_size=0.3) #train 70% - test 30%
# data_clus_dt

"""# ***THUẬT TOÁN K-MEANS***
# Bước 1. Scaled
# Bước 2. Khởi tạo tâm cụm ngẫu nhiên
# Bước 3. Dãn nhãn cho mỗi điểm dữ liệu
# Bước 4. Cập nhật tâm cụm
# Bước 5. Lập bước 3 và 4 cho đến khi tâm cụm không thay đổi


"""

from scipy.spatial.distance import cdist
from sklearn.cluster import KMeans

losses = []
K = 10
for i in range(1, K):
  # 1.  Huấn luyện với số cụm = i
  kmeans_i = KMeans(n_clusters=i, random_state=0)
  kmeans_i.fit(data_clus_dt)
  # 2. Tính _hàm biến dạng_
  # 2.1. Khoảng cách tới toàn bộ centroids
  d2centroids = cdist(data_clus_dt, kmeans_i.cluster_centers_, 'euclidean') # shape (n, k)
  # 2.2. Khoảng cách tới centroid gần nhất
  min_distance = np.min(d2centroids, axis=1) # shape (n)
  loss = np.sum(min_distance)
  losses.append(loss)

plt.figure(figsize=(12, 8))
plt.plot(np.arange(1, K), losses, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Distortion')
plt.title('The Elbow Method using Distortion')
plt.show()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_X = scaler.fit_transform(data_clus_dt)

data_clus_dt.describe()

# Bước 2: Khởi tạo tâm cụm ngẫu nhiên
import pandas as pd
import numpy as np
def random_centroids(data, k):
  centroids = []
  for i in range(k):
    centroid  = data.apply(lambda x: float(x.sample()))
    centroids.append(centroid)
  return pd.concat(centroids, axis=1)

centroids = random_centroids(data_clus_dt, 2)
centroids

# Bước 2: Dãn nhán cho mỗi điểm dữ liệu
def get_labels(data, centroids):
  distances = centroids.apply(lambda x: np.sqrt(((data - x) ** 2).sum(axis=1)))
  return distances.idxmin(axis=1)
labels = get_labels(data, centroids)
labels.value_counts()

#Bước 3. Cập nhật tâm cụm
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from IPython.display import clear_output
def new_centroids(data, labels, k):
  return data.groupby(labels).apply(lambda x: np.exp(np.log(x).mean())).T
def plotcluster(data, labels, centroids, iteration):
  pca = PCA(n_components=3)
  data_2d = pca.fit_transform(data)
  centroids_2d = pca.transform(centroids.T)
  clear_output(wait=True)
  plt.title(f'Iteration {iteration}')
  plt.scatter(x=data_2d[:,0], y=data_2d[:,1], c=labels)
  plt.scatter(x=centroids_2d[:,0], y=centroids_2d[:,1])
  plt.show()

# Bước 4. Lập bước 2 và 3 cho đến khi tâm cụm không thay đổi
from sklearn.impute import SimpleImputer
max_iterations = 100
k=2
centroids = random_centroids(data, k)
old_centroids = pd.DataFrame()
iteration = 1

while iteration < max_iterations and not centroids.equals(old_centroids):
  old_centroids = centroids
  labels = get_labels(data, centroids)
  centroids = new_centroids(data, labels, k)
  plotcluster(data, labels, centroids, iteration)
  iteration += 1

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, random_state=0, init="k-means++")
kmeans.fit(data_clus_dt)

#Y_pred_km=kmeans.predict(X_test_clus_dt)
kmeans.labels_

# Đánh giá độ phân cụm của thuật toán K_Means bằng DAVIES BOULDIN
from sklearn import metrics
Accuracy_KMeans=round((metrics.davies_bouldin_score(data_clus_dt, kmeans.labels_ )*1.00),2)
print('Accuracy: ',Accuracy_KMeans)
Accuracy["KMeans"]=Accuracy_KMeans

"""# **THUẬT TOÁN DBSCAN**"""

#X_dbdf= df.loc[:, ['Sulfate','Solids']].values

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# fit là phương thức xây dựng
#fit_transform xây dựng và chuyển đổi
scaled_X = scaler.fit_transform(data_clus_dt)
scaled_X

from sklearn.cluster import DBSCAN
number_of_outliers =[]
percentage_of_outliers =[]

min_samples = 2 * scaled_X.shape[1]

for eps in np.linspace(0.001, 3, 50):
  dbscan = DBSCAN(eps=eps, min_samples=min_samples)
  dbscan.fit(scaled_X)

  number_of_outliers.append(np.sum(dbscan.labels_== -1))
 

  perc_outliers = 100 * np.sum(dbscan.labels_ == -1) / len(dbscan.labels_)
  percentage_of_outliers.append(perc_outliers)
  # a=(dbscan.labels_==1)  TÌM THUỘC TÍNH LABELS (DỮ LIỆU NHIỄU) SO SÁNH NÓ CÓ BẰNG TRỪ 1 HAY K NẾU CÓ SẼ TÍNH TỔNG NÓ VÀ THÊM VÀO MẢNG
  # print("a",a)
  # x=np.sum(dbscan.labels_)
  # print("x",x)
  # y=len(dbscan.labels_) 3114 LÀ ĐỘ DÀI CỦA CÁC HÀNG
  # print("y",y)
  # print("z=",percentage_of_outliers)

sns.lineplot(x=np.linspace(0.001, 3, 50), y=percentage_of_outliers);

plt.xlabel("Epsilon Value")
plt.ylabel("Percentage of Outliers");

from sklearn.cluster import DBSCAN
min_samples= 2* scaled_X.shape[1]
dbscan = DBSCAN(eps=0.5, min_samples= min_samples)
dbscan.fit(scaled_X)

plt.figure(figsize=(6,5), dpi=150)
sns.scatterplot(data=data_clus_dt,x='Sulfate',y='Solids',hue=dbscan.labels_,palette='Set1')
#plt.savefig("db.png", format='png', dpi=100, bbox_inches='tight')

from sklearn.cluster import DBSCAN
min_samples= 2* scaled_X.shape[1]
dbscan = DBSCAN(eps=0.5, min_samples= min_samples).fit(scaled_X)
dbscan.labels_
#Y_pred_db=dbscan.fit_predict(X_test_clus_dt)

from sklearn import metrics
Accuracy_DBscan=round((metrics.davies_bouldin_score(data_clus_dt, dbscan.labels_ )*1.00),2)
print('Accuracy: ',Accuracy_DBscan)
Accuracy["KMeans"]=Accuracy_DBscan

#Visualize mô hình ROC (Receiver Operating Characteristic) Curves - so sánh trực quan các mô hình phân loại
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, accuracy_score

plt.figure(figsize=(12,9))
models = [
{
    'label': 'Decision Tree',
    'model': clf,
},
{
    'label': 'Random Forest',
    'model': rdf,
},
{
    'label': 'Support Vector Machine',
    'model': svm,
}
]

for m in models:
    model = m['model'] 
    model.fit(X_train_dt, Y_train_dt) 
    y_pred=model.predict(X_test_dt) 
    fpr, tpr, thresholds = roc_curve(Y_test_dt, model.predict_proba(X_test_dt)[:,1])
    auc = roc_auc_score(Y_test_dt,model.predict(X_test_dt))
    plt.plot(fpr, tpr, linestyle='-', linewidth=3, label='%s ROC (area = %0.2f)' % (m['label'], auc))

plt.plot([0, 1], [0, 1],'b--', linewidth=3)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1 - Specificity (False Positive Rate)', fontsize=15)
plt.ylabel('Sensitivity (True Positive Rate)', fontsize=15)
plt.title('Receiver Operating Characteristic (ROC)', fontsize=15)
plt.legend(loc="lower right", fontsize=14)
#plt.savefig("roc_curves.png", format='png', dpi=100, bbox_inches='tight')
plt.show()

